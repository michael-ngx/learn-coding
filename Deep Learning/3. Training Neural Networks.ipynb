{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "## 1. Hyperparameters\n",
    "\n",
    "Weights and bias are ones we can use gradients to update\n",
    "\n",
    "BUT, there are many other different aspects of a neural network that we can change (<mark>**hyperparameters**</mark>):\n",
    "\n",
    "- Batch size\n",
    "- Number of layers\n",
    "- Layer size\n",
    "- Type of activation function\n",
    "- Learning rate\n",
    "\n",
    "$\\implies$ Nested loop:\n",
    "\n",
    "- Outer loop of optimization: hyperparameters (number of iterations)\n",
    "- Inner loop of optimization: weights, bias\n",
    "\n",
    "![Alt text](images/img24.png)\n",
    "\n",
    "***** There are so many different layer configurations, **how do we search for the more optimized models?**\n",
    "\n",
    "<mark>**We look at configurations which include all hyperparameters**</mark>\n",
    "\n",
    "There are 2 ways to tune hyperparameters\n",
    "\n",
    "- Grid search (enumerate through all configurations) --> very expensive\n",
    "- **Random search** (we can limit to 20 random models) --> More common\n",
    "    - Create a space based on limits\n",
    "    - Randomly run models, and pick the one with the best validation\n",
    "\n",
    "![Alt text](images/img25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Optimizers\n",
    "\n",
    "An optimizer is an algorithm that, based on the value of the **loss function**, adjusts how each parameter (weight/learning rate) should change.\n",
    "\n",
    "The optimizer solves the **credit assignment problem**: how do we assign credit (or blame) to the parameters based on how the network performs?\n",
    "\n",
    "<mark>All neural network optimizers in this course will be based on **gradient descent**.</mark>\n",
    "\n",
    "**PyTorch** automates the gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- For each iteration evaluate a training sample from the dataset **taken at random**\n",
    "- **Computing the gradient takes less time**, but... may not actually be faster.\n",
    "- Optimization path that looks rather erratic\n",
    "- SGD allows you to do more of a **global search** for an optimum, often results in a better set of weights for your model\n",
    "- GD on entire training data!\n",
    "\n",
    "![Alt text](images/img26.png)\n",
    "\n",
    "Red: using only 1 example, then 1 example --> **SGD**\n",
    "- Slower, less stable: Gradient jumps around more and more (cat, then dog)\n",
    "- BUT: more randomness\n",
    "\n",
    "Black: using all datas at once --> **Batch Gradient Descent**\n",
    "- Faster, more stable\n",
    "- BUT: will go towards a mean, which will aggregate and **drop a lot of information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Mini-Batch Gradient Descent</mark> (Combination of the two)\n",
    "\n",
    "- Instead of working with one sample at a time... can apply **batching**...\n",
    "\n",
    "> - Use our network to make the predictions for **n samples**\n",
    "> - Compute the average loss for those **n samples**\n",
    "> - Take optimize the average loss of those **n samples**, means finishing an iteration\n",
    "\n",
    "**Batch size** (n): Number of training examples used per optimization “step” a.k.a iteration.  \n",
    "**Iteration**: One step: The parameters are updated once per iteration.  \n",
    "**Epoch**: Number of times all the train data is used once to update the parameters  \n",
    "\n",
    "Suppose there are 1000 samples in train data, batch size = 20 $\\implies$ 1 epoch contain 50 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent: N-Dimensional\n",
    "\n",
    "A deep neural network has millions or billions of parameters  \n",
    "\n",
    "<mark>**Real gradient descent of a deep network is optimization in millions of dimensions!**</mark>\n",
    "\n",
    "Most points of zero gradients are saddle points.  \n",
    "Plateaus are a problem but can be addressed using specialized variants on gradient descent  \n",
    "\n",
    "Prob [all dimensions have gradient = 0] ~ 0\n",
    "\n",
    "It's possible that we're minima in 1 dimension, and maxima in another dimension (**saddle point**)\n",
    "\n",
    "--> Can easily skip it (since we are trying to minimize for all dimensions)\n",
    "\n",
    "Plateaus can easily be addressed as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SGD with Momentum**\n",
    "\n",
    "<mark>**Ravines**</mark>: areas where the surface curves much more steeply in one dimension than in another, common around **local optima**.\n",
    "\n",
    "SGD has trouble navigating ravines → it oscillates across the slopes of the ravine --> SLOW\n",
    "\n",
    "Momentum helps **accelerate** SGD in the relevant direction and dampens oscillations SGD\n",
    "\n",
    "<img src=\"images/img28.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "The momentum term increases for dimensions whose gradients point in the same  \n",
    "directions and reduces updates for dimensions whose gradients change directions\n",
    "\n",
    "Analogy → we push a ball down a hill. The ball accumulates momentum as it rolls downhill,  \n",
    "becoming faster and faster on the way until it reaches its terminal velocity\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    v_{ji}^t = \\lambda v_{ji}^{t-1} - \\gamma \\frac{\\partial E}{\\partial w_{ji}^{t}} \\\\\n",
    "    w_{ji}^t = w_{ji}^{t-1} + v_{ji}^t\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Initially (previous formula), $\\lambda$ is 0. Then, if $\\lambda$ is set to be non 0, movement is accelerated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Adaptive Moment Estimation (Adam)</mark>\n",
    "\n",
    "$\\implies$ USE FOR OPTIMIZER (Very stable and give good results)\n",
    "\n",
    "Adaptive learning rates → <mark>**each weight has its own rate**</mark>, instead of a fixed $\\gamma$\n",
    "\n",
    "$$m_t = \\beta _1 m_{t-1} + (1 - \\beta _1)({\\partial E \\over \\partial w_{ji}})$$\n",
    "$$v_t = \\beta _2 v_{t-1} + (1 - \\beta _2)({\\partial E \\over \\partial w_{ji}})^2$$\n",
    "$$w_{ji}^{t+1} = w_{ji}^{t} - ({\\gamma \\over \\sqrt{v_t} + \\epsilon})m_t$$\n",
    "\n",
    "**The added denominator is dependent on gradient of each weight**\n",
    "\n",
    "$({\\gamma \\over \\sqrt{v_t} + \\epsilon})$ is the **new learning rate**\n",
    "\n",
    "**** This incorporates momentum and adaptive learning rate (different rate per weight)\n",
    "\n",
    "- rapid convergence\n",
    "- requires minimal tuning\n",
    "- commonly used optimizer\n",
    "\n",
    "    `torch.optim.Adam(model.parameters(),lr=0.001)`\n",
    "    \n",
    "\n",
    "<img src=\"images/img29.png\" width=\"20%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Learning Rate ($\\gamma$)\n",
    "\n",
    "The learning rate determines the **size of the step** that an optimizer takes during each iteration\n",
    "\n",
    "$$w_{ji}^{t+1} = w_{ji}^{t} - \\gamma \\frac{\\partial E}{\\partial w_{ji}}$$\n",
    "\n",
    "Learning rate size is also important, and depends on many different things:\n",
    "- The learning problem\n",
    "- The optimizer\n",
    "- The batch size\n",
    "    - Large batch → larger learning rates.\n",
    "    - Small batch → smaller learning rate.\n",
    "- The stage of training\n",
    "    - Reduce as training progresses\n",
    "\n",
    "<img src=\"images/img30.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "$\\implies$ **Begin with large learning rate, then decreases** (learning rate scheduling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Normalization\n",
    "\n",
    "We always normalize the inputs to **prevent the model from paying attention to the features with larger range (magnitude)**.\n",
    "\n",
    "First layer:\n",
    "- $\\mu$ = average of X across samples\n",
    "- $\\sigma$ = standard deviation across samples\n",
    "\n",
    "<img src=\"images/img31.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "Each layer for the next layers: (output of this layer also needs to be normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Batch Normalization\n",
    "\n",
    "Normalize activations batch-wise for each layer\n",
    "\n",
    "<img src=\"images/img32.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "**Inference Time (application time)**: We're given just the input data, and the model is trained with normalization and batch\n",
    "\n",
    "THEREFORE, we need to add something. **Keep a moving average** of $\\mu$ and $\\sigma$ during training <mark>**(Step 5)**</mark>, so we can use it at inference time\n",
    "\n",
    "<img src=\"images/img33.png\" width=\"90%\" height=\"90%\">\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Higher learning rate → speeding up the training\n",
    "- Regularizes the model\n",
    "- Less sensitivity to initialization\n",
    "\n",
    "Cons:\n",
    "- Depends on batch size → No effect with small batches\n",
    "- Cannot work with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>4.2 Layer Normalization</mark>\n",
    "\n",
    "USE THIS \n",
    "\n",
    "**Normalization is applied on the neuron** for a single instance across all features **in each example**\n",
    "\n",
    "- Simpler to implement, no moving averages or parameters\n",
    "- Not dependent on batch size\n",
    "\n",
    "<img src=\"images/img34.png\" width=\"20%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Regularization\n",
    "\n",
    "Techniques to **avoid overfit**, making the model hard to memorize by making the problem more difficult\n",
    "\n",
    "### 5.1 Dropout\n",
    "\n",
    "Forces a neural network to learn more robust features\n",
    "- **During training** → In each training step: Drop activations (neurons) (set to 0) with probability p\n",
    "    - e.g. Each neuron has p% of being kept/drop\n",
    "    - The network **must adapt and learn based on the underlying distribution of data rather than memorizing**\n",
    "- **During inference** → multiply weights by (1-p) to keep the same distribution\n",
    "    - During distribution, we have the full network\n",
    "    - *For example, a neuron have p% chance to receive K' inputs out of K. Therefore, during inference, we must multiply each of the K inputs to the neuron by (1 - p), to get something closer to K'*\n",
    "\n",
    "![Alt text](images/img35.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Weight Decay (L2)\n",
    "\n",
    "Prevents the weights from growing too much → **Lowering variance**\n",
    "\n",
    "Add a term to the loss function (magnitude of the entire weight matrix) $\\implies$ **The magnitude of the weights should not get too big**\n",
    "\n",
    "Weight reduction is multiplicative and proportion to the scale of W\n",
    "\n",
    "<img src=\"images/img36.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Early Stopping with Patience\n",
    "\n",
    "Recall: we should stop the training once we observe that the validation error is increasing.  \n",
    "However, this will sometimes miss the more optimal results\n",
    "\n",
    "- In each training iteration observe the validation loss\n",
    "- As soon as validation loss starts to increase, start a counter\n",
    "- If the validation loss decreases, reset the counter\n",
    "- Otherwise, wait for a fixed iterations (patience) and then stop the training\n",
    "\n",
    "<img src=\"images/img37.png\" width=\"50%\" height=\"50%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
