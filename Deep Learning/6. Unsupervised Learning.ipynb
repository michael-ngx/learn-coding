{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerating new images:\n",
    "\n",
    "Basically mean **we would want the network to save space by mapping similar images to similar embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are done training, we remove the decoder\n",
    "\n",
    "For each image, we have the embedding\n",
    "\n",
    "--> Now given 2 images from training data, we can pass through the encoder, which gives us 2 embeddings\n",
    "\n",
    "Now I can interpolate between 2 embeddings\n",
    "\n",
    "e.g. e3 = alpha e1 + (1 - alpha) e2\n",
    "\n",
    "Now pass this e3 through Decoder, and will generate image X3 that's not from the data (result in something alpha% similar to image 1, 1-alpha% similar to image 2)\n",
    "\n",
    "WHAT IF we create a random embedding and pass it through encoder?\n",
    "- Lucky: random embedding will be close to 1 meaningful image (low probability)\n",
    "- Usual: get something nonsense \n",
    "\n",
    "--> How can we have a **true generative model**: pass in random numbers and generate a meaningful image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Variational AutoEncoders (VAE)\n",
    "\n",
    "Variant of autoencoders -> Used to generate new images \n",
    "\n",
    "Different:\n",
    "- Models so far is deterministic. Once we trained, everytime we pass in 1 input, we get 1 output\n",
    "- **Probabilistic**: 1 input, different output at different times\n",
    "- **Generative**: can generate new instances that look like they were sampled from the training set\n",
    "\n",
    "They impose a distribution constraint on the latent space to have a smooth space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEs not learning single static embedding per input, we learn a **distribution over embeddings**. Assume that this is normal/gaussian distribution\n",
    "\n",
    "The only thing we require to define these distributions are $\\mu$ (mean of data) and $\\sigma$ (std deviation of data)\n",
    "\n",
    "--> Only need to learn 2 variable. Therefore, the encoder will generate **(learning) 2 vectors of mu and sigma.**\n",
    "\n",
    "What we're going to use is to use these mu and sigma to sample and get the embedding\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-25 at 10.18.29 AM.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Sampling is a non-differentiable operation. Once we sample, there's no gradient. So how do we train the network?\n",
    "\n",
    "--> Use **reparametrization trick**\n",
    "\n",
    "$$\\text{z = mu + sigma * epsilon}$$\n",
    "\n",
    "$$\\text{epsilon is random sampling, got from N(0, I)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Convolutional Autoencoders\n",
    "\n",
    "Remember: convolution is much better fit for images\n",
    "\n",
    "- **Encoder** part: we already know (similar to Convolutional Classifier)\n",
    "- **Decoder** part: once we get to the embedding, we want to go in the **reverse direction**\n",
    "\n",
    "### Transposed Convolution\n",
    "\n",
    "Increasing resolution rather than decreasing it\n",
    "\n",
    "$$o = (i - 1) * s + (k - 1) - 2p + op + 1$$\n",
    "\n",
    "- o: output dimension\n",
    "- i: input dimension\n",
    "- s: stride (only for the output, consider all input pixels)\n",
    "- k: kernel size\n",
    "- p: input padding\n",
    "- op: output padding\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-25 at 10.44.26 AM.png\" height=30% width=30%>\n",
    "\n",
    "### Padding: drop output border\n",
    "\n",
    "### Output padding\n",
    "\n",
    "- Previous convolution may map multiple inputs to one output\n",
    "- So, output padding is needed for transpose convolution to identify exactly which dimension we want\n",
    "\n",
    "\n",
    "TECHNICALLY, if we pass the input -> conv -> transpose conv -> input again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 7)\n",
    "    )\n",
    "    self.decoder = nn.Sequential(           # A function that packs multiple layers together\n",
    "        nn.ConvTranspose2d(64, 32, 7),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1,output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "    return x\n",
    "def embed(self, x):\n",
    "    return self.encoder(x)\n",
    "def decode(self, e):\n",
    "    return self.decode(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Pre-training with Autoencoders\n",
    "\n",
    "Recall: we can do Transfer learning in CNN\n",
    "\n",
    "We can do the exact same thing with autoencoders.\n",
    "\n",
    "Once we are done training, we can **drop decoder**, attach a classifier to classify your data :) \n",
    "\n",
    "--> Only need very small amount of supervised data (with label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
