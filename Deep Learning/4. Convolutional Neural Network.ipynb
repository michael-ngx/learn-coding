{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "### Motivation:\n",
    "\n",
    "- MNIST dataset: input is cleaned, all 28x28, handwriting is in center of image $\\implies$ what if image is **diff size, distorted**?\n",
    "- Very hard to make NN bigger, since there will be so many parameters to train\n",
    "- **Bad inductive bias**: When flatten data, this ignores geometry of the image (BAD)\n",
    "- Not flexible: different image size require different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolution Operator (in 2D)\n",
    "\n",
    "Different types of <mark>**kernels**</mark> are used to **extract a layer of info**\n",
    "- Low-level info\n",
    "    - Verticle edge detector\n",
    "    - Horizontal edge detector, etc.\n",
    "- High-level info\n",
    "    - Head? Nose? Ears?\n",
    "\n",
    "Calculations are like dot product between elements $\\implies$ **results in a scalar for each operation**\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-24 at 6.50.07 PM.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "These kernels used to be **hand-crafted**, meaning their values are **set**, and then applied to classify images.\n",
    "\n",
    "HOWEVER, this will not work for distorted images and even small tweaks $\\implies$ we need something better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Convolutional Neural Networks\n",
    "\n",
    "<mark>**CNN learns the kernel values**</mark>\n",
    "\n",
    "- Weight sharing: detech the same local features across the image\n",
    "- Locally connected layers: local features in small regions of the image\n",
    "\n",
    "**All parameters are trained end-to-end (updated at the same time)**\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-24 at 7.13.26 PM.png\">\n",
    "\n",
    "- Last layers are classifying strong features, which is then fed into fully connected network (as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminologies\n",
    "\n",
    "- <mark>**Zero padding**</mark>: Adding zeros to border of image before convolution (avoid bias towards center of image)\n",
    "- <mark>**Stride**</mark>: Distance between 2 consecutive positions of the kernel $\\implies$ **controls output resolution**\n",
    "\n",
    "$$\\text{Output size} = \\lfloor {i + 2p + k \\over s} \\rfloor + 1$$\n",
    "\n",
    "- i = image dimension (ixi)\n",
    "- k = kernel size (kxk)\n",
    "- p = padding size\n",
    "- s = stride size\n",
    "\n",
    "**NOTE**: If input size is m x n and kernel size is k x l: Output size is $O_w$ x $O_h$ where:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    O_w = \\lfloor {m + 2p + k \\over s} \\rfloor + 1\\\\\n",
    "    O_h = \\lfloor {n + 2p + l \\over s} \\rfloor + 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on RGB\n",
    "\n",
    "Imagine image will now have **depth = 3**, then kernel **depth = 3**\n",
    "\n",
    "$$\\text{Kernel depth always = Input depth}$$\n",
    "$$\\text{Each kernel layer always produces output depth 1}$$\n",
    "\n",
    "ALSO, we might want to include multiple **types of kernels** in each NN layer, extracting different information.\n",
    "\n",
    "$\\begin{align}\n",
    "&\\text{Number of input channels = input depth} \\\\\n",
    "&\\text{Number of output channels = number of kernels} \\\\\n",
    "&\\text{Number of trainable weights = number of kernels * img depth (kernel depth) * img resolution}\n",
    "\\end{align}$\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-25 at 6.40.30 PM.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Pooling Operator\n",
    "\n",
    "**Consolidate information**, removing ones that are not useful (the deeper we go into layers)\n",
    "\n",
    "### <mark>Max Pooling</mark>\n",
    "\n",
    "Extracts the max value within a region\n",
    "\n",
    "$$\\text{Output size} = \\lfloor {i - k \\over s} \\rfloor + 1$$\n",
    "\n",
    "### <mark>Average Pooling</mark>\n",
    "\n",
    "Extracts the average value within a region (IN PRACTICE: max pooling generally works better)\n",
    "\n",
    "$$\\text{Output size} = \\lfloor {i + 2p + k \\over s} \\rfloor + 1 \\text{(same as normal convolution)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. CNN in PyTorch\n",
    "\n",
    "### <mark>CNN Architecture Blueprint</mark>\n",
    "\n",
    "- Filter **depth increases** (more kernels per layer)\n",
    "    - For the first layers, we are only extracting low-level features that are simple   \n",
    "    - The more deep we go, complex kernels are required to combine low-level features to extract high-level features\n",
    "\n",
    "- Feature map **height and width decreases** (image resolution decreases)\n",
    "    - In low-level, we only learn simple fetures that can be extracted in small regions of image (e.g. simple edge)\n",
    "    - The higher-level requires capturing bigger area of the image (e.g. the entire face, the whole body)\n",
    "    - So to keep the kernel size constant, we shrink the image to help the kernel capture meaningful information\n",
    "\n",
    "<img src=\"images/Screenshot 2023-10-24 at 7.43.10 PM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeNet(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(LargeNet, self)._init_()\n",
    "        self.name = \"Large\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)  # linearize\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Why can we define 1 pool, but use 2 times?\n",
    "- Doesn't depend on input size (pool is just arithmetic operation)\n",
    "- Pool has NO WEIGHTS TO LEARN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
