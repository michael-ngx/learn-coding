{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Pytorch Implementation\n",
    "\n",
    "### MNIST Dataset\n",
    "\n",
    "Input: 28x28 pixel image (gray scale)\n",
    "\n",
    "Output: Binary classification\n",
    "- output = 1: if digit is small (0, 1, 2)\n",
    "- output = 0: otherwise (3,4,5,6,7,8,9)\n",
    "\n",
    "![Alt text](images/img38.png)\n",
    "\n",
    "Supervised or unsupervised? \n",
    "- Supervised (since we have access to labels)\n",
    "- Classification: Outputs are categorical and not continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN in PyTorch\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt     # for plotting\n",
    "import torch.optim as optim         # for optimizer\n",
    "torch.manual_seed(1)                # (when developing:) set the random seed to a fixed model, Important for reproducing results\n",
    "                                    # so that we can see whether a change is caused by random seed or by editting configurations\n",
    "                                    # same number when we call the random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our NN architecture\n",
    "\n",
    "- We usually use a class, inherit from nn.Module\n",
    "    - class nn.Linear defines a fully-connected layer\n",
    "\n",
    "- **forward()** method defines how to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a 2-layer neural network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pigeon, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 30)    # 30 is just hyper param. This means input x is (28x28x1), w is (30x28x28), b is (30x1)\n",
    "        self.layer2 = nn.Linear(30, 1)          # inputs and output. 1 since we're doing binary classification\n",
    "        \n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = self.layer1(flattened)\n",
    "        activation1 = F.relu(activation1)\n",
    "        activation2 = self.layer2(activation1)\n",
    "        return activation2\n",
    "        \n",
    "model = NN()        # instantiated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: where is the final function to map the result to probability? (output is a linear layer)\n",
    "\n",
    "### Loss Function and Softmax Activation\n",
    "\n",
    "- You would expect to see the softmax activation applied to the output layer. Indeed this would be the case if we used:\n",
    "\n",
    "    `criterion = nn.BCELoss()`\n",
    "\n",
    "- Due to numerical stability, we will use:\n",
    "\n",
    "    `criterion = nn.BCEWithLogitsLoss()`       --> <mark>RECOMMENDED</mark>\n",
    "\n",
    "This applies **softmax activation internally**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Load Data\n",
    "\n",
    "Load MNIST data: (there are also other data loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load commonly used MNIST dataset\n",
    "mnist_data = datasets.MNIST('data', train=True, download=True)\n",
    "mnist_data = list(mnist_data)\n",
    "mnist_train = mnist_data[:1000]     # training data\n",
    "mnist_val = mnist_data[1000:2000]   # validation data\n",
    "img_to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Pass\n",
    "\n",
    "Forward pass: **makes a prediction**\n",
    "- e.g. `model(input)`, which calls network.forward method\n",
    "- Information flows forwards from input to output layer\n",
    "\n",
    "Backward pass: **computes gradients** for making changes to weights\n",
    "- e.g. `loss.backward()`\n",
    "- Information flows backwards from output to input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training code for binary classification problems\n",
    "\n",
    "# define loss function and optimizer settings\n",
    "criterion = nn.BCEWithLogitsLoss()          # NOTE: this is why we do not need softmax in the end\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)  # or adam instead of SGD\n",
    "\n",
    "for (image, label) in mnist_train:\n",
    "    # get ground truth: is the digit less than 3?\n",
    "    actual = torch.tensor(label < 3).reshape([1,1]).type(torch.FloatTensor)\n",
    "    \n",
    "    out = model(img_to_tensor(image))       # make prediction (through all layers)\n",
    "    loss = criterion(out, actual)           # calculate loss\n",
    "    loss.backward()                         # obtain gradients -> Get all gradients for all parameters in model (all layers)\n",
    "    optimizer.step()                        # updates parameters -> Update all weights and biases \n",
    "                                                # based on SGD with lr and momentum (throughout all layers)\n",
    "    optimizer.zero_grad()                   # a clean up step - important! This sets gradient to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Error\n",
    "\n",
    "**NOW, the model is trained** based on mnist_train.\n",
    "\n",
    "We can assess model performance by tracking error rate and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the error and accuracy on the training set\n",
    "error = 0\n",
    "for (image, label) in mnist_train:\n",
    "    prob = torch.sigmoid(model(img_to_tensor(image)))                   # For each image, pass it to sigmoid to get prediction\n",
    "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):      # assume that class 0 < 0.5, class 1 >= 0.5\n",
    "        error += 1\n",
    "print(\"Training Error Rate:\", error/len( mnist_train))\n",
    "print(\"Training Accuracy:\", 1 - error/len( mnist_train))\n",
    "\n",
    "# computing the error and accuracy on the validation set\n",
    "error = 0\n",
    "for (image, label) in mnist_val:\n",
    "    prob = torch.sigmoid(model(img_to_tensor(image)))\n",
    "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
    "        error += 1\n",
    "print(\"Testing Error Rate:\", error/len( mnist_val))\n",
    "print(\"Testing Accuracy:\", 1 - error/len( mnist_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION:\n",
    "\n",
    "- model(img_to_tensor(image)) generates output (that is previously used against \"actual\" to compute loss). Why do we need to pass it through torch.sigmoid again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Multi-Class Classification\n",
    "\n",
    "Identify each image to its corresponding digit 0-9\n",
    "\n",
    "Do One-hot encoding\n",
    "\n",
    "Requires minor changes to our PyTorch:\n",
    "\n",
    "1. The final output layer has **as many neurons as classes.**\n",
    "2. Apply the **softmax activation function** on the final layer to obtain class probabilities\n",
    "3. Use the **multiclass cross-entropy** loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 50)    # 50, 20 and 3 layers are hyperparameter choice\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 10)         # 1 output neuron for each digit number\n",
    "        \n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = F.relu(self.layer1(flattened))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output                       # NO SOFTMAX FUNCTION here?\n",
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Softmax Activation\n",
    "\n",
    "You would expect to see the softmax activation applied to the output layer. Indeed this would be the case if we used:\n",
    "\n",
    "    `criterion = nn.NLLLoss()`\n",
    "\n",
    "Due to numerical stability, we will use: (**returns the loss, but has a softmax embedded**)\n",
    "\n",
    "    `criterion = nn.CrossEntropyLoss()`     --> RECOMMENDED\n",
    "\n",
    "$\\implies$ Applies softmax activation internally!\n",
    "\n",
    "### Output Probabilities\n",
    "\n",
    "To obtain output probabilities we have to apply the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = F.softmax(output, dim=1)\n",
    "print(prob)\n",
    "print(sum(prob[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation and Debugging\n",
    "\n",
    "### Debugging Neural Networks\n",
    "\n",
    "- Make sure your model can overfit\n",
    "    - Make sure you can get loss to decrease w.r.t training data\n",
    "- Make sure that your network is training: i.e. loss is going down.\n",
    "    - Sanity check!\n",
    "- Ensures that you are using the right variable names, and rule out other  \n",
    "programming bugs that are difficult to discern from architecture issues.\n",
    "- Confusion Matrix\n",
    "    - True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN)\n",
    "- 2D Projections of Data\n",
    "    - PCA, t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>**Confusion Matrix**</mark>\n",
    "\n",
    "(If we have 10 classes, we will have a 10x10 matrix)\n",
    "\n",
    "The correct predictions are on the diagonal\n",
    "\n",
    "![Alt text](images/img39.png)\n",
    "\n",
    "**NOTE**: This accuracy formula is only correct if the input is distributed equally. **ELSE, IT IS BAD**\n",
    "\n",
    "> e.g We want to predict cancer, minority class have cancer, majority don't have.  \n",
    "> If this formula gives high prediction rate for people that DO NOT have cancer, but do badly for people who HAVE cancer  \n",
    "> the accuracy is still high, but we might still **built a bad model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>**F1 Score**</mark>\n",
    "\n",
    "Good for imbalance datasets\n",
    "\n",
    "![Alt text](images/img40.png)\n",
    "\n",
    "<mark>**Precision**</mark>: How many True Positives (predicted positive, is positive) out of Predicted Positive  \n",
    "\n",
    "<mark>**Recall**</mark>: How many True Positives (predicted positive, is positive) out of Is Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 2D Visualization\n",
    "\n",
    "If visualization shows clear separation between data, which means **your model is doing well**\n",
    "\n",
    "![Alt text](images/img41.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
