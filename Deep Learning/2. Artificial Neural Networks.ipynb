{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artificial Neuron\n",
    "\n",
    "![Alt text](images/image-2.png)\n",
    "\n",
    "- **x<sub>i</sub>** is the **input** (axes of features to data)(can have multi dimensions: price, size, color, etc.)\n",
    "- **w<sub>i</sub>** is the **weight** for input x<sub>i</sub> that we learn for this particular input (multiplied with each dimension, and sum them up)\n",
    "- **b** is the **bias** (scalar), a weight we learn with no input\n",
    "- **f** is the **activation function** that determines how our output changes with the sum of all weight-input products\n",
    "- **y** is the **output** such as the class an image belongs to\n",
    "\n",
    "What are we training here? --> THE **w** and the **b**\n",
    "\n",
    "In practice, we are representing x and w as vectors and matrix to do multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Activation Function\n",
    "\n",
    "### Linear Activation Function (Separating 2 classes)\n",
    "\n",
    "Suppose the activation function is a simple linear function\n",
    "\n",
    "$$y = w * x + b$$\n",
    "\n",
    "A line --> <mark>Decision boundary</mark> is the \"line\" that separates 2 sets of data that our task is trying to do\n",
    "\n",
    "![Alt text](images/image-3.png)\n",
    "\n",
    "(y = 1 or y = -1, not the result of activation function) is 2 separated classes --> Then, if output of activation function > 0, y = 1. Else, y = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron with a Linear Activation Function\n",
    "\n",
    "*What's **wrong** with a linear activation function?*\n",
    "\n",
    "- Most real datasets are not linearly separable, e.g. we can't find a line that separates classes well in a classification problem \n",
    "- We can learn non-linear transformations of our data to help (activation function is sin, cos, etc.)  \n",
    "- Multiple layers with non-linear transformations help\n",
    "- <mark>**No advantage from multiple linear layers**</mark> (since then, the composite of layers is a linear layer)\n",
    "\n",
    "$$ w3(w2(w1*x + b1) + b2) + b3 = (w3w2w1) * x + (w3w2b1 + w3b2 + b3) = w' * x + b' $$\n",
    "\n",
    "![Alt text](images/image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Activation Functions: Perceptrons\n",
    "\n",
    "Biological neurons only either output/no output --> these guys where following exactly this model (heaviside (unit) step function)\n",
    "\n",
    "--> This is called the decision boundary\n",
    "\n",
    "**Problem**: These functions are not differentiable, continuous, or smooth\n",
    "\n",
    "![Alt text](images/img3.png)\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "\n",
    "> We rely on derivatives and gradients to find signals for **w** and **b**\n",
    "\n",
    "- Easily differentiable, smooth, continuous\n",
    "- Range between [-1, 1] or [0, 1]\n",
    "\n",
    "There are many sigmoid functions, the most common are:\n",
    "\n",
    "$$f(x) = tanh(x)$$\n",
    "$$f(x) = {1 \\over 1 + e^{-x}}$$\n",
    "\n",
    "**Problem**: Saturated neurons “kill” the gradients. Gradients become **vanishingly small very quickly** away from x=0  \n",
    "\n",
    "--> In most cases we do not have any signals to train our model (since the derivative is just 0)\n",
    "\n",
    "![Alt text](images/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function (MODERN)\n",
    "\n",
    "<mark>Rectified Linear Unit (ReLU)</mark> based activation functions:\n",
    "\n",
    "ReLU\n",
    "\n",
    "$$ReLU(x) = (x)^+ = max(0,x)$$\n",
    "\n",
    "LeakyReLU\n",
    "\n",
    "$$\n",
    "LeakyReLU(x) = \n",
    "\\begin{cases}\n",
    "    x & \\text{if } x > 0 \\\\\n",
    "    negativeslope * x & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Parametric ReLU\n",
    "\n",
    "$$\n",
    "PReLU(x) = \n",
    "\\begin{cases}\n",
    "    x & \\text{if } x > 0 \\\\\n",
    "    ax & \\text{otherwise (some constant a)} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ReLU have **very easy derivatives (either 0 or 1), use derivative = 0 at x = 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img7.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "**** If we have billions of neurons, each with this function, given that the derivative is always 0 or 1.\n",
    "\n",
    "**** Then, we can approximate any function using many small lines (data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Training Artificial Neurons (Supervised Learning)\n",
    "\n",
    "So, how to we \"train\" the **w** and **b** based on the prediction error?\n",
    "\n",
    "> input: **x**, predicted output: y, gound truth label (correct output): t, Neuron M(**w;x**)\n",
    "\n",
    "(in other words, we have y = M(w;x) and we want to match this as close as possible to t)\n",
    "\n",
    "> 1. Make a prediction for some input data x, with a known correct output t\n",
    ">\n",
    ">           y = M(w;x)\n",
    ">\n",
    "> 2. Compare the correct output with our predicted output to compute **loss** (<mark>loss function</mark>):\n",
    ">\n",
    ">           E = Loss(y, t)\n",
    "> \n",
    "> 3. **Adjust the weights/bias** to make the prediction closer to the ground truth, i.e. minimize error\n",
    ">\n",
    "> 4. **Repeat** until we have an acceptable level of error\n",
    "\n",
    "#### **Inference time**\n",
    "\n",
    "Now, after these 4 steps in the training process, we will use the **w** and **b** to **predict the output of new x**\n",
    "\n",
    "#### **<mark>Forward pass and Backward pass</mark>**\n",
    "\n",
    "- \"Forward pass\" refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer. A loss function is calculated from the output values.\n",
    "- \"Backward pass\" refers to process of counting changes in weights using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n",
    "\n",
    "Backward and forward pass makes together one \"iteration\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 4. Loss Function\n",
    "\n",
    "A loss function computes how bad **predictions are compared to the ground truth labels**.\n",
    "\n",
    "- Large loss: the network’s prediction differs from the ground truth\n",
    "- Small loss: the network’s prediction matches the ground truth\n",
    "\n",
    "We want to calculate the error over **all training samples (average error)**\n",
    "\n",
    "![Alt text](images/img9.png)\n",
    "\n",
    "<mark>Each iteration is called an Epoch</mark>\n",
    "\n",
    "As we can see, the model will learn better as we iterate more  \n",
    "**BUT, the validation result will become worse after a while (it starts to memorize the model, and will lose generalization)**  \n",
    "even though training error keeps decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/img10.png)\n",
    "\n",
    "Here, we want to classify 3 classes --> we use 3 neurons\n",
    "\n",
    "The **x** column input, in this case for example, is just the representation of the input (vectorized version of pixels) (colors, feature, etc.)\n",
    "\n",
    "### Interpreting the answer:\n",
    "\n",
    "- The prediction is the one with the highest score (obviously)\n",
    "- How can we output the **<mark>confidence</mark> of the answer?**\n",
    "\n",
    "<mark>**logits**</mark> = raw output before normalization\n",
    "\n",
    "<mark>**Softmax function**</mark> → normalizes the logits into a categorical probability distribution over all possible classes.\n",
    "\n",
    "![Alt text](images/img11.png)\n",
    "\n",
    "This outputs probabilities (confidence) of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We cannot compare \"437.9\" to cat, how do we do so?\n",
    "\n",
    "**One-hot encoding** --> Maps categories to vector representation at the beginning **(ground truth label)**\n",
    "\n",
    "![Alt text](images/img12.png)\n",
    "\n",
    "THEN, apply **Cross Entropy (CE)** --> Mostly used for **classification problems**\n",
    "\n",
    "![Alt text](images/img14.png)\n",
    "\n",
    "THEN, apply **Mean Squared Error** --> Mostly used for **regression problems**\n",
    "\n",
    "![Alt text](images/img13.png)\n",
    "\n",
    "NOTE: What we use for comparing errors are normalized --> we are not omitting information by just using the class of the expected class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/img15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Pass with Error Calculations\n",
    "\n",
    "(Normally we would use pandas instead of doing for loops - this is just to illustrate)\n",
    "\n",
    "NOTE: This example is classifying 2 classes (either 0 or 1). 4 ground truth labels --> 4 examples (inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x = [[1.0, 0.1,-0.2],   # data\n",
    "    [1.0,-0.1, 0.9],\n",
    "    [1.0, 1.2, 0.1],\n",
    "    [1.0, 1.1, 1.5]]\n",
    "t = [0, 0, 0, 1]        # labels\n",
    "w = [1, -1, 1]          # initial weights\n",
    "\n",
    "# x(4 x 3) * w(3 x 1) = y(4 x 1) -> Compared with t\n",
    "\n",
    "def simple_ANN(x, w, t):\n",
    "    e_bce = []\n",
    "    e_mse = []\n",
    "    y = []\n",
    "    for n in range(len(x)):\n",
    "        v = 0\n",
    "        for d in range(len(x[0])):\n",
    "            v += x[n][d] * w[d]\n",
    "        y.append(1/(1+math.e**(-v))) # sigmoid\n",
    "        e_bce.append(-t[n]*math.log(y[n])-(1-t[n])*math.log(1-y[n]))\n",
    "        e_mse.append((y[n]-t[n])**2)\n",
    "    \n",
    "    total_e_bce = sum(e_bce)/len(x) # average error - BCE\n",
    "    total_e_mse = sum(e_mse)/len(x) # average error - MSE\n",
    "    return ((y, w, total_e_bce) , (y, w, total_e_mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Gradient Descent\n",
    "\n",
    "### Neural Network Layer (Vector, Matrices, and Tensors)\n",
    "\n",
    "A neural network layer with two neurons:\n",
    "\n",
    "$$y1 = f(w1*x+b1)$$\n",
    "$$y2 = f(w2*x+b2)$$\n",
    "\n",
    "Say N = number of characteristics  \n",
    "M = number of neurons (classes to be classified)\n",
    "\n",
    "Can represent NN layer easier with a weight matrix,  \n",
    "e.g. where **each neuron's weight vector is a row** of the weight matrix W (M x N)  \n",
    "and the **input is a column vector x**: (N x 1)\n",
    "\n",
    "$$\\implies y = f(Wx + b)$$\n",
    "\n",
    "is M x 1 (result for each class)\n",
    "\n",
    "![Alt text](images/img27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer training: Delta rule\n",
    "\n",
    "**How do we edit each of our neuron's weights $w_{ji}$ to reduce E (at every step)?**\n",
    "\n",
    "--> Through <mark>**derivatives (gradient)**</mark>\n",
    "\n",
    "Vector of partial derivatives for all weights is the **gradient**\n",
    "- Direction of the gradient is the direction in which the function increases most quickly\n",
    "- Magnitude of the gradient is the rate of increase\n",
    "\n",
    "Adjusting weights according to the slope (gradient) will guide us the minimum (or maximum) error\n",
    "\n",
    "![Alt text](images/img17.png)\n",
    "\n",
    "![Alt text](images/img18.png)\n",
    "\n",
    "In higher dimensional space (Deep Learning), the probability of finding (stucking in) a **local minima** will be very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Rule for Single Weight/Training Sample\n",
    "\n",
    "Using **chain rule**\n",
    "\n",
    "In this example:\n",
    "\n",
    "- E = f(y) = MSE formula\n",
    "- y = f(a) = Sigmoid activation function\n",
    "- a = weight * x + b\n",
    "\n",
    "![Alt text](images/img19.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-Pass & Backward-Pass\n",
    "\n",
    "def simple_ANN(x, w, t, iter, lr):\n",
    "    '''x: input\n",
    "    w: weight\n",
    "    t: ground truth\n",
    "    iter: num iterations (learn iter times)\n",
    "    lr: learning rate (w new = w old + lr * dE/dw)'''\n",
    "    \n",
    "    for i in range(iter):\n",
    "        e, y = [], []\n",
    "        for n in range(len(x)):\n",
    "            v = 0\n",
    "            for d in range(len(x[0])):\n",
    "                v += x[n][d] * w[d]\n",
    "            y.append(1/(1+math.e**(-v))) # sigmoid\n",
    "            e.append((y[n]-t[n])**2) # MSE\n",
    "            \n",
    "            # gradient descent to update weights\n",
    "            for p in range(len(w)):\n",
    "                d = 2*x[n][p]*(y[n]-t[n])*(1-y[n])*y[n]\n",
    "                w[p] -= lr*d\n",
    "    total_e = sum(e)/len(x)\n",
    "    return (y, w, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Neural Network Architectures\n",
    "\n",
    "### Multiple Layers are Important: XOR\n",
    "\n",
    "Having a single decision boundary (a single NN layer) is not enough to solve many problems\n",
    "\n",
    "The most famous such problem is the XOR function, which needs two decision boundaries to solve\n",
    "\n",
    "We solve this by having **at least 2 neural network layer** (1 hidden)\n",
    "\n",
    "In fact in the limit of an infinitely-wide neural network with at least one hidden layer, NN is a **universal function approximator**\n",
    "\n",
    "![Alt text](images/img21.png)\n",
    "\n",
    "### Backpropagation: Solving Credit Assignment Problem\n",
    "\n",
    "When the last layer receives feedback, the credit asignment problem arises when it **doesn't know which previous steps needs updating**\n",
    "\n",
    "$\\implies$ Use DP to calculate intermediate steps for gradient calculations (save into a look up table)\n",
    "\n",
    "- **Backpropagation** is a way of propagating the total loss back into the NN to know how much of the loss every node is responsible for, and updating those weights\n",
    "\n",
    "![Alt text](images/img22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers with Non-Linearity\n",
    "\n",
    "Each layer is a projection from one space to another\n",
    "\n",
    "$\\implies$ Neural networks can take in a data, project to another space so many times such that **in the final space, a linearity can be used to separate your data**  \n",
    "$\\implies$ So in the final space, normally we only use linear activation function\n",
    "\n",
    "![Alt text](images/img20.png)\n",
    "\n",
    "NN can be viewed as a way of **learning features** directly and end-to-end from raw input data\n",
    "\n",
    "Each layer (before the last layer) have **activations** that can be used as **high-level features** representing input data\n",
    "\n",
    "![Alt text](images/img23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed-Forward Network**: Information only flows forward from one layer to a later layer, from the input to the output.\n",
    "\n",
    "**Fully-Connected Network**: Neurons between adjacent layers are fully connected.\n",
    "\n",
    "**Number of Layers**: Number of hidden layers + output layer.\n",
    "\n",
    "An architecture of a NN describes neurons and their **connectivity**\n",
    "\n",
    "Selection of architecture greatly affect performance.\n",
    "\n",
    "In future weeks we will introduce more NN architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
